{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fs7Xtr5H5eRB",
        "outputId": "0301dca0-1f6a-4124-c828-1b3a3460a008"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive/', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9aECeEy6ra0",
        "outputId": "4feb70ed-307b-48b2-b416-cd41bee83439"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive\n"
          ]
        }
      ],
      "source": [
        "%cd /content/gdrive/MyDrive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUiwkRv-XAkW"
      },
      "source": [
        "#**Download MARBERT checkpoint**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8wBRd5zXAkX",
        "outputId": "e04d960b-1b22-4d98-ba1a-8ed03fdc2c73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-25 18:56:34--  https://huggingface.co/UBC-NLP/MARBERT/resolve/main/MARBERT_pytorch_verison.tar.gz\n",
            "Resolving huggingface.co (huggingface.co)... 18.172.170.14, 18.172.170.44, 18.172.170.36, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.172.170.14|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/UBC-NLP/MARBERT/85bfec76f38cba4bc2e6cd02a959016de37ba93de4c850a7d175811dce4e8adc?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27MARBERT_pytorch_verison.tar.gz%3B+filename%3D%22MARBERT_pytorch_verison.tar.gz%22%3B&response-content-type=application%2Fgzip&Expires=1685300195&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL1VCQy1OTFAvTUFSQkVSVC84NWJmZWM3NmYzOGNiYTRiYzJlNmNkMDJhOTU5MDE2ZGUzN2JhOTNkZTRjODUwYTdkMTc1ODExZGNlNGU4YWRjP3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiZyZXNwb25zZS1jb250ZW50LXR5cGU9KiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY4NTMwMDE5NX19fV19&Signature=eyAqyq5484fy-kbzxmxhLUihSH%7Egc-2si13NNR3yfA%7ERCymUtHxF5AeGZhdVhSi1OLR5eo3TyAPaTIdjX9dwJT8gcx4hr-8VCqLX6EaoPhxWu7OHVcVGuu7jGiPGNnNI2XKhppYcx7h49KaCrv3KhhmgXGRNu%7EmIrhkqE4HhSLIMAsPyBiM2etW25bI5L885HY6lak8jaUXJ8aB8Ny1Ob1WQvOd8DpaYxZW0a704Ess6v2LYcsxeI%7EWOSu78fktUgSGIxfWjXLP1n-YruD8uxIXD5eTfgKuEkyDlN3H2UErGFLeCpDrpafTb657Wk9Hmktyh%7E0oXOSHrBWOe5d0htg__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-05-25 18:56:34--  https://cdn-lfs.huggingface.co/UBC-NLP/MARBERT/85bfec76f38cba4bc2e6cd02a959016de37ba93de4c850a7d175811dce4e8adc?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27MARBERT_pytorch_verison.tar.gz%3B+filename%3D%22MARBERT_pytorch_verison.tar.gz%22%3B&response-content-type=application%2Fgzip&Expires=1685300195&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL1VCQy1OTFAvTUFSQkVSVC84NWJmZWM3NmYzOGNiYTRiYzJlNmNkMDJhOTU5MDE2ZGUzN2JhOTNkZTRjODUwYTdkMTc1ODExZGNlNGU4YWRjP3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiZyZXNwb25zZS1jb250ZW50LXR5cGU9KiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY4NTMwMDE5NX19fV19&Signature=eyAqyq5484fy-kbzxmxhLUihSH%7Egc-2si13NNR3yfA%7ERCymUtHxF5AeGZhdVhSi1OLR5eo3TyAPaTIdjX9dwJT8gcx4hr-8VCqLX6EaoPhxWu7OHVcVGuu7jGiPGNnNI2XKhppYcx7h49KaCrv3KhhmgXGRNu%7EmIrhkqE4HhSLIMAsPyBiM2etW25bI5L885HY6lak8jaUXJ8aB8Ny1Ob1WQvOd8DpaYxZW0a704Ess6v2LYcsxeI%7EWOSu78fktUgSGIxfWjXLP1n-YruD8uxIXD5eTfgKuEkyDlN3H2UErGFLeCpDrpafTb657Wk9Hmktyh%7E0oXOSHrBWOe5d0htg__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.65.229.73, 18.65.229.16, 18.65.229.83, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.65.229.73|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 607066087 (579M) [application/gzip]\n",
            "Saving to: ‘MARBERT_pytorch_verison.tar.gz.7’\n",
            "\n",
            "MARBERT_pytorch_ver 100%[===================>] 578.94M  65.8MB/s    in 8.4s    \n",
            "\n",
            "2023-05-25 18:56:43 (69.0 MB/s) - ‘MARBERT_pytorch_verison.tar.gz.7’ saved [607066087/607066087]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://huggingface.co/UBC-NLP/MARBERT/resolve/main/MARBERT_pytorch_verison.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FYvgJevXAkY",
        "outputId": "d2470212-ae79-47a7-ec8d-f21d76a97725"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MARBERT_pytorch_verison/\n",
            "MARBERT_pytorch_verison/pytorch_model.bin\n",
            "MARBERT_pytorch_verison/config.json\n",
            "MARBERT_pytorch_verison/vocab.txt\n"
          ]
        }
      ],
      "source": [
        "!tar -xvf MARBERT_pytorch_verison.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnYaiKpVEMtG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2pkLcyfXAka",
        "outputId": "642d3b41-9bef-486b-d362-6f54068e25b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting GPUtil\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytorch_pretrained_bert\n",
            "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (1.22.4)\n",
            "Collecting boto3 (from pytorch_pretrained_bert)\n",
            "  Downloading boto3-1.26.141-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (4.65.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (2022.10.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m106.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=0.4.1->pytorch_pretrained_bert) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=0.4.1->pytorch_pretrained_bert) (16.0.5)\n",
            "Collecting botocore<1.30.0,>=1.29.141 (from boto3->pytorch_pretrained_bert)\n",
            "  Downloading botocore-1.29.141-py3-none-any.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3->pytorch_pretrained_bert)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0 (from boto3->pytorch_pretrained_bert)\n",
            "  Downloading s3transfer-0.6.1-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (3.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.30.0,>=1.29.141->boto3->pytorch_pretrained_bert) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.1->pytorch_pretrained_bert) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=0.4.1->pytorch_pretrained_bert) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.141->boto3->pytorch_pretrained_bert) (1.16.0)\n",
            "Building wheels for collected packages: GPUtil\n",
            "  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7393 sha256=60a575d73d568dec1c3f5d2f711a4319bbd087a78da5b3a3cc0c00c74dbeebdd\n",
            "  Stored in directory: /root/.cache/pip/wheels/a9/8a/bd/81082387151853ab8b6b3ef33426e98f5cbfebc3c397a9d4d0\n",
            "Successfully built GPUtil\n",
            "Installing collected packages: tokenizers, GPUtil, jmespath, huggingface-hub, botocore, transformers, s3transfer, boto3, pytorch_pretrained_bert\n",
            "Successfully installed GPUtil-1.4.0 boto3-1.26.141 botocore-1.29.141 huggingface-hub-0.14.1 jmespath-1.0.1 pytorch_pretrained_bert-0.6.2 s3transfer-0.6.1 tokenizers-0.13.3 transformers-4.29.2\n"
          ]
        }
      ],
      "source": [
        "!pip install GPUtil pytorch_pretrained_bert transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xBm7opGXAkb"
      },
      "source": [
        "# Fine-tuning code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oOXwzeXXAkb"
      },
      "outputs": [],
      "source": [
        "# (1)load libraries \n",
        "import json, sys, regex\n",
        "import torch\n",
        "import GPUtil\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.utils import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertConfig, BertAdam, BertForSequenceClassification\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report, confusion_matrix\n",
        "##----------------------------------------------------\n",
        "from transformers import *\n",
        "from transformers import XLMRobertaConfig\n",
        "from transformers import XLMRobertaModel\n",
        "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
        "from transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer, XLMRobertaModel\n",
        "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from transformers import AutoTokenizer, AutoModel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQWKMrXPXAkc",
        "outputId": "5a37a663-bdd5-415d-dd26-50bcf1e59655"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "your device  cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print (\"your device \", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKujHwr5XAkd"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_label2ind_file(file, label_col):\n",
        "\tlabels_json={}\n",
        "\t#load train_dev_test file\n",
        "\tdf = pd.read_csv(file, sep=\"\\t\")\n",
        "\tdf.head(5)\n",
        "\t#get labels and sort it A-Z\n",
        "\tlabels = df[label_col].unique()\n",
        "\tlabels.sort()\n",
        "\t#convert labels to indexes\n",
        "\tfor idx in range(0, len(labels)):\n",
        "\t\tlabels_json[labels[idx]]=idx\n",
        "\tprint(labels_json)\n",
        "\t#save labels with indexes to file\n",
        "\twith open(label2idx_file, 'w') as json_file:\n",
        "\t\tjson.dump(labels_json, json_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNZxr8m6XAkd"
      },
      "outputs": [],
      "source": [
        "def data_prepare_BERT(file_path, lab2ind, tokenizer, content_col, label_col, MAX_LEN):\n",
        "\t# Use pandas to load dataset\n",
        "\tdf = pd.read_csv(file_path, delimiter='\\t', header=0)\n",
        "\tdf = df[df[content_col].notnull()]\n",
        "\tdf = df[df[label_col].notnull()]\n",
        "\tprint(\"Data size \", df.shape)\n",
        "\t# Create sentence and label lists\n",
        "\tsentences = df[content_col].values\n",
        "\tsentences = [\"[CLS] \" + sentence+ \" [SEP]\" for sentence in sentences]\n",
        "\tprint (\"The first sentence:\")\n",
        "\tprint (sentences[0])\n",
        "\t# Create sentence and label lists\n",
        "\tlabels = df[label_col].values\n",
        "\t#print (labels)\n",
        "\tlabels = [lab2ind[i] for i in labels]\n",
        "\t# Import the BERT tokenizer, used to convert our text into tokens that correspond to BERT's vocabulary.\n",
        "\ttokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "\tprint (\"Tokenize the first sentence:\")\n",
        "\tprint (tokenized_texts[0])\n",
        "\t#print(\"Label is \", labels[0])\n",
        "\t# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "\tinput_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "\tprint (\"Index numbers of the first sentence:\")\n",
        "\tprint (input_ids[0])\n",
        "\t# Pad our input seqeunce to the fixed length (i.e., max_len) with index of [PAD] token\n",
        "\t# ~ input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\tpad_ind = tokenizer.convert_tokens_to_ids(['[PAD]'])[0]\n",
        "\tinput_ids = pad_sequences(input_ids, maxlen=MAX_LEN+2, dtype=\"long\", truncating=\"post\", padding=\"post\", value=pad_ind)\n",
        "\tprint (\"Index numbers of the first sentence after padding:\\n\",input_ids[0])\n",
        "\t# Create attention masks\n",
        "\tattention_masks = []\n",
        "\t# Create a mask of 1s for each token followed by 0s for padding\n",
        "\tfor seq in input_ids:\n",
        "\t\tseq_mask = [float(i > 0) for i in seq]\n",
        "\t\tattention_masks.append(seq_mask)\n",
        "\t# Convert all of our data into torch tensors, the required datatype for our model\n",
        "\tinputs = torch.tensor(input_ids)\n",
        "\tlabels = torch.tensor(labels)\n",
        "\tmasks = torch.tensor(attention_masks)\n",
        "\treturn inputs, labels, masks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdPCPv8VXAke"
      },
      "outputs": [],
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "# def flat_accuracy(preds, labels):\n",
        "#\t  pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "#\t  labels_flat = labels.flatten()\n",
        "#\t  return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "def flat_pred(preds, labels):\n",
        "\tpred_flat = np.argmax(preds, axis=1).flatten()\n",
        "\tlabels_flat = labels.flatten()\n",
        "\treturn pred_flat.tolist(), labels_flat.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vyvc8JoXAke"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train(model, iterator, optimizer, scheduler, criterion):\n",
        "\t\n",
        "\tmodel.train()\n",
        "\tepoch_loss = 0\n",
        "\tfor i, batch in enumerate(iterator):\n",
        "\t\t# Add batch to GPU\n",
        "\t\tbatch = tuple(t.to(device) for t in batch)\n",
        "\t\t# Unpack the inputs from our dataloader\n",
        "\t\tinput_ids, input_mask, labels = batch\n",
        "\t\toutputs = model(input_ids, input_mask, labels=labels)\n",
        "\t\tloss, logits = outputs[:2]\n",
        "\t\t#print(\"loss \",loss)\n",
        "\t\t# delete used variables to free GPU memory\n",
        "\t\tdel batch, input_ids, input_mask, labels\n",
        "\t\toptimizer.zero_grad()\n",
        "\t\tif torch.cuda.device_count() == 1:\n",
        "\t\t\tloss.backward()\n",
        "\t\t\tepoch_loss += loss.cpu().item()\n",
        "\t\telse:\n",
        "\t\t\tloss.sum().backward()\n",
        "\t\t\tepoch_loss += loss.sum().cpu().item()\n",
        "\t\toptimizer.step()\n",
        "\t\t#print(\"optimizer step\")\n",
        "\t\ttorch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)  # Gradient clipping is not in AdamW anymore\n",
        "\t\t# optimizer.step()\n",
        "\t\tscheduler.step()\n",
        "\t# free GPU memory\n",
        "\tif device == 'cuda':\n",
        "\t\ttorch.cuda.empty_cache()\n",
        "\tprint(\"will return now\")\n",
        "\treturn epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hjwa6a3bXAkf"
      },
      "outputs": [],
      "source": [
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "\tmodel.eval()\n",
        "\tepoch_loss = 0\n",
        "\tall_pred=[]\n",
        "\tall_label = []\n",
        "\twith torch.no_grad():\n",
        "\t\tfor i, batch in enumerate(iterator):\n",
        "\t\t\t# Add batch to GPU\n",
        "\t\t\tbatch = tuple(t.to(device) for t in batch)\n",
        "\t\t\t# Unpack the inputs from our dataloader\n",
        "\t\t\tinput_ids, input_mask, labels = batch\n",
        "\t\t\toutputs = model(input_ids, input_mask, labels=labels)\n",
        "\t\t\tloss, logits = outputs[:2]\n",
        "\t\t\t# delete used variables to free GPU memory\n",
        "\t\t\tdel batch, input_ids, input_mask\n",
        "\t\t\tif torch.cuda.device_count() == 1:\n",
        "\t\t\t\tepoch_loss += loss.cpu().item()\n",
        "\t\t\telse:\n",
        "\t\t\t\tepoch_loss += loss.sum().cpu().item()\n",
        "\t\t\t# identify the predicted class for each example in the batch\n",
        "\t\t\tprobabilities, predicted = torch.max(logits.cpu().data, 1)\n",
        "\t\t\t# put all the true labels and predictions to two lists\n",
        "\t\t\tall_pred.extend(predicted)\n",
        "\t\t\tall_label.extend(labels.cpu())\n",
        "\taccuracy = accuracy_score(all_label, all_pred)\n",
        "\tf1score = f1_score(all_label, all_pred, average='macro') \n",
        "\trecall = recall_score(all_label, all_pred, average='macro')\n",
        "\tprecision = precision_score(all_label, all_pred, average='macro')\n",
        "\treport = classification_report(all_label, all_pred)\n",
        "\treturn (epoch_loss / len(iterator)), accuracy, f1score, recall, precision\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nWmg6P-XAkf"
      },
      "outputs": [],
      "source": [
        "\n",
        "def fine_tuning(config):\n",
        "\t#---------------------------------------\n",
        "\tprint (\"[INFO] step (1) load train_test config file\")\n",
        "\t# config_file = open(config_file, 'r', encoding=\"utf8\")\n",
        "\t# config = json.load(config_file)\n",
        "\ttask_name = config[\"task_name\"]\n",
        "\tcontent_col = config[\"content_col\"]\n",
        "\tlabel_col = config[\"label_col\"]\n",
        "\ttrain_file = config[\"data_dir\"]+config[\"train_file\"]\n",
        "\tdev_file = config[\"data_dir\"]+config[\"dev_file\"]\n",
        "\tsortby = config[\"sortby\"]\n",
        "\tmax_seq_length= int(config[\"max_seq_length\"])\n",
        "\tbatch_size = int(config[\"batch_size\"])\n",
        "\tlr_var = float(config[\"lr\"])\n",
        "\tmodel_path = config['pretrained_model_path']\n",
        "\tnum_epochs = config['epochs'] # Number of training epochs (authors recommend between 2 and 4)\n",
        "\tglobal label2idx_file\n",
        "\tlabel2idx_file = config[\"data_dir\"]+config[\"task_name\"]+\"_labels-dict.json\"\n",
        "\t#-------------------------------------------------------\n",
        "\tprint (\"[INFO] step (2) convert labels2index\")\n",
        "\tcreate_label2ind_file(train_file, label_col)\n",
        "\tprint (label2idx_file)\n",
        "\t#---------------------------------------------------------\n",
        "\tprint (\"[INFO] step (3) check checkpoint directory and report file\")\n",
        "\tckpt_dir = config[\"data_dir\"]+task_name+\"_bert_ckpt/\"\n",
        "\treport = ckpt_dir+task_name+\"_report.tsv\"\n",
        "\tsorted_report = ckpt_dir+task_name+\"_report_sorted.tsv\"\n",
        "\tif not os.path.exists(ckpt_dir):\n",
        "\t\tos.mkdir(ckpt_dir)\n",
        "\t#-------------------------------------------------------\n",
        "\tprint (\"[INFO] step (4) load label to number dictionary\")\n",
        "\tlab2ind = json.load(open(label2idx_file))\n",
        "\tprint (\"[INFO] train_file\", train_file)\n",
        "\tprint (\"[INFO] dev_file\", dev_file)\n",
        "\tprint (\"[INFO] num_epochs\", num_epochs)\n",
        "\tprint (\"[INFO] model_path\", model_path)\n",
        "\tprint (\"max_seq_length\", max_seq_length, \"batch_size\", batch_size)\n",
        "\t#-------------------------------------------------------\n",
        "\tprint (\"[INFO] step (5) Use defined funtion to extract tokanize data\")\n",
        "\t# tokenizer from pre-trained BERT model\n",
        "\tprint (\"loading BERT setting\")\n",
        "\ttokenizer = BertTokenizer.from_pretrained(model_path)\n",
        "\ttrain_inputs, train_labels, train_masks = data_prepare_BERT(train_file, lab2ind, tokenizer,content_col, label_col, max_seq_length)\n",
        "\tvalidation_inputs, validation_labels, validation_masks = data_prepare_BERT(dev_file, lab2ind, tokenizer, content_col, label_col,max_seq_length)\n",
        "\t# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top.\n",
        "\tmodel = BertForSequenceClassification.from_pretrained(model_path, num_labels=len(lab2ind))\n",
        "\t#--------------------------------------\n",
        "\tprint (\"[INFO] step (6) Create an iterator of data with torch DataLoader.\")\n",
        "#\t\t  This helps save on memory during training because, unlike a for loop,\\\n",
        "#\t\t  with an iterator the entire dataset does not need to be loaded into memory\")\n",
        "\ttrain_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "\ttrain_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
        "\t#---------------------------\n",
        "\tvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "\tvalidation_dataloader = DataLoader(validation_data, batch_size=batch_size)\n",
        "\t#------------------------------------------\n",
        "\tprint (\"[INFO] step (7) run with parallel GPUs\")\n",
        "\tif torch.cuda.is_available():\n",
        "\t\tif torch.cuda.device_count() == 1:\n",
        "\t\t\tprint(\"Run\", \"with one GPU\")\n",
        "\t\t\tmodel = model.to(device)\n",
        "\t\telse:\n",
        "\t\t\tn_gpu = torch.cuda.device_count()\n",
        "\t\t\tprint(\"Run\", \"with\", n_gpu, \"GPUs with max 4 GPUs\")\n",
        "\t\t\tdevice_ids = GPUtil.getAvailable(limit = 4)\n",
        "\t\t\ttorch.backends.cudnn.benchmark = True\n",
        "\t\t\tmodel = model.to(device)\n",
        "\t\t\tmodel = nn.DataParallel(model, device_ids=device_ids)\n",
        "\telse:\n",
        "\t\tprint(\"Run\", \"with CPU\")\n",
        "\t\tmodel = model\n",
        "\t#---------------------------------------------------\n",
        "\tprint (\"[INFO] step (8) set Parameters, schedules, and loss function\")\n",
        "\tglobal max_grad_norm\n",
        "\tmax_grad_norm = 1.0\n",
        "\twarmup_proportion = 0.1\n",
        "\tnum_training_steps\t= len(train_dataloader) * num_epochs\n",
        "\tnum_warmup_steps = num_training_steps * warmup_proportion\n",
        "\t### In Transformers, optimizer and schedules are instantiated like this:\n",
        "\t# Note: AdamW is a class from the huggingface library\n",
        "\t# the 'W' stands for 'Weight Decay\"\n",
        "\toptimizer = AdamW(model.parameters(), lr=lr_var, correct_bias=False)\n",
        "\t# schedules\n",
        "\tscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)  # PyTorch scheduler\n",
        "\t# We use nn.CrossEntropyLoss() as our loss function. \n",
        "\tcriterion = nn.CrossEntropyLoss()\n",
        "\t#---------------------------------------------------\n",
        "\tprint (\"[INFO] step (9) start fine_tuning\")\n",
        "\tfor epoch in trange(num_epochs, desc=\"Epoch\"):\n",
        "\t\ttrain_loss = train(model, train_dataloader, optimizer, scheduler, criterion)\t  \n",
        "\t\tval_loss, val_acc, val_f1, val_recall, val_precision = evaluate(model, validation_dataloader, criterion)\n",
        "\t\tprint (train_loss, val_acc)\n",
        "\t\t# Create checkpoint at end of each epoch\n",
        "\t\tif not os.path.exists(ckpt_dir + 'model_' + str(int(epoch + 1)) + '/'): os.mkdir(ckpt_dir + 'model_' + str(int(epoch + 1)) + '/')\n",
        "\t\tmodel.save_pretrained(ckpt_dir+ 'model_' + str(int(epoch + 1)) + '/')\n",
        "\t\tepoch_eval_results = {\"epoch_num\":int(epoch + 1),\"train_loss\":train_loss,\n",
        "\t\t\t\t\t  \"val_acc\":val_acc, \"val_recall\":val_recall, \"val_precision\":val_precision, \"val_f1\":val_f1,\"lr\":lr_var }\n",
        "\t\twith open(report,\"a\") as fOut:\n",
        "\t\t\tfOut.write(json.dumps(epoch_eval_results)+\"\\n\")\n",
        "\t\t\tfOut.flush()\n",
        "\t\t#------------------------------------\n",
        "\t\treport_df = pd.read_json(report, orient='records', lines=True)\n",
        "\t\treport_df.sort_values(by=[sortby],ascending=False, inplace=True)\n",
        "\t\treport_df.to_csv(sorted_report,sep=\"\\t\",index=False)\n",
        "\treturn report_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDnDyOFHiZvk"
      },
      "source": [
        "# **Control Experiment for 1 Epoch**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ol6LaSAAXAki"
      },
      "outputs": [],
      "source": [
        "\n",
        "config={\"task_name\": \"AJGT_MARBERT\", #output directory name\n",
        "             \"data_dir\": \"./\", #data directory\n",
        "             \"train_file\": \"ar_rev_train.tsv\", #train file path\n",
        "             \"dev_file\": \"ar_rev_test.tsv\", #dev file path or test file path\n",
        "             \"pretrained_model_path\": 'MARBERT_pytorch_verison', #MARBERT checkpoint path\n",
        "             \"epochs\": 1, #number of epochs\n",
        "             \"content_col\": \"text\", #text column\n",
        "             \"label_col\": \"label\", #label column\n",
        "             \"lr\": 2e-06, #learning rate\n",
        "              \"max_seq_length\": 128, #max sequance length\n",
        "              \"batch_size\": 32, #batch shize\n",
        "              \"sortby\":\"val_acc\"} #sort results based on val_acc or val_f1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jq9kOFoaXAkj",
        "outputId": "53b2e077-f673-4e4b-8977-a3ebf0bd0396"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] step (1) load train_test config file\n",
            "[INFO] step (2) convert labels2index\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading file vocab.txt\n",
            "loading file added_tokens.json\n",
            "loading file special_tokens_map.json\n",
            "loading file tokenizer_config.json\n",
            "loading configuration file MARBERT_pytorch_verison/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Mixed': 0, 'Negative': 1, 'Positive': 2}\n",
            "./AJGT_MARBERT_labels-dict.json\n",
            "[INFO] step (3) check checkpoint directory and report file\n",
            "[INFO] step (4) load label to number dictionary\n",
            "[INFO] train_file ./ar_rev_train.tsv\n",
            "[INFO] dev_file ./ar_rev_test.tsv\n",
            "[INFO] num_epochs 1\n",
            "[INFO] model_path MARBERT_pytorch_verison\n",
            "max_seq_length 128 batch_size 32\n",
            "[INFO] step (5) Use defined funtion to extract tokanize data\n",
            "loading BERT setting\n",
            "Data size  (79999, 2)\n",
            "The first sentence:\n",
            "[CLS] رواية بسيطه عن مغترب تونسي يزور عائلته في اجازه قصيره ويلمس تغير البلاد والافكار الدينيه التي غزت العقول وحجم التناقض الكبير بين القول والفعل ..رواية جميله على عكس روايته السابقه روائح ماري كلير .. [SEP]\n",
            "Tokenize the first sentence:\n",
            "['[CLS]', 'رواية', 'بسيطه', 'عن', 'مغترب', 'تونسي', 'يزور', 'عايلته', 'في', 'اجازه', 'قصيره', 'ويلم', '##س', 'تغير', 'البلاد', 'والافكار', 'الدينيه', 'التي', 'غز', '##ت', 'العقول', 'وحجم', 'التناقض', 'الكبير', 'بين', 'القول', 'والفعل', '.', '.', 'رواية', 'جميله', 'على', 'عكس', 'رواي', '##ته', 'السابقه', 'روايح', 'ماري', 'كلي', '##ر', '.', '.', '[SEP]']\n",
            "Index numbers of the first sentence:\n",
            "[2, 12268, 13009, 1986, 43299, 38656, 12203, 30919, 1947, 12042, 18794, 66258, 1019, 3969, 6812, 40324, 65586, 2367, 9461, 1009, 7849, 53462, 26215, 4935, 2199, 6915, 52491, 16, 16, 12268, 5551, 1977, 8124, 68846, 2088, 30577, 61165, 18682, 5248, 1010, 16, 16, 3]\n",
            "Index numbers of the first sentence after padding:\n",
            " [    2 12268 13009  1986 43299 38656 12203 30919  1947 12042 18794 66258\n",
            "  1019  3969  6812 40324 65586  2367  9461  1009  7849 53462 26215  4935\n",
            "  2199  6915 52491    16    16 12268  5551  1977  8124 68846  2088 30577\n",
            " 61165 18682  5248  1010    16    16     3     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0]\n",
            "Data size  (20000, 2)\n",
            "The first sentence:\n",
            "[CLS] أقم على الماشي . بصورة عامة الغرفة جيدة وكل شيء متوفر فيها. عدم وجود ستارة الحمام [SEP]\n",
            "Tokenize the first sentence:\n",
            "['[CLS]', 'اقم', 'على', 'الماشي', '.', 'بصورة', 'عامة', 'الغرفة', 'جيدة', 'وكل', 'شيء', 'متوفر', 'فيها', '.', 'عدم', 'وجود', 'ستار', '##ة', 'الحمام', '[SEP]']\n",
            "Index numbers of the first sentence:\n",
            "[2, 32246, 1977, 43114, 16, 6543, 8484, 21889, 13442, 2910, 2265, 11233, 2408, 16, 4113, 3715, 10709, 1046, 12169, 3]\n",
            "Index numbers of the first sentence after padding:\n",
            " [    2 32246  1977 43114    16  6543  8484 21889 13442  2910  2265 11233\n",
            "  2408    16  4113  3715 10709  1046 12169     3     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file MARBERT_pytorch_verison/config.json\n",
            "Model config BertConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.29.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "loading weights file MARBERT_pytorch_verison/pytorch_model.bin\n",
            "Some weights of the model checkpoint at MARBERT_pytorch_verison were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at MARBERT_pytorch_verison and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] step (6) Create an iterator of data with torch DataLoader.\n",
            "[INFO] step (7) run with parallel GPUs\n",
            "Run with one GPU\n",
            "[INFO] step (8) set Parameters, schedules, and loss function\n",
            "[INFO] step (9) start fine_tuning\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "will return now\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Configuration saved in ./AJGT_MARBERT_bert_ckpt/model_1/config.json\n",
            "Model weights saved in ./AJGT_MARBERT_bert_ckpt/model_1/pytorch_model.bin\n",
            "Epoch: 100%|██████████| 1/1 [31:56<00:00, 1916.04s/it]\n"
          ]
        }
      ],
      "source": [
        "report_df = fine_tuning(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "5V9ZOdVNXAkk",
        "outputId": "9c04067f-16c5-43e8-c431-d122d99b746e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-810a122b-1520-468b-9519-d9382c2e990e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>epoch_num</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>val_acc</th>\n",
              "      <th>val_recall</th>\n",
              "      <th>val_precision</th>\n",
              "      <th>val_f1</th>\n",
              "      <th>lr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0.668333</td>\n",
              "      <td>0.727</td>\n",
              "      <td>0.72752</td>\n",
              "      <td>0.72458</td>\n",
              "      <td>0.724107</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-810a122b-1520-468b-9519-d9382c2e990e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-810a122b-1520-468b-9519-d9382c2e990e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-810a122b-1520-468b-9519-d9382c2e990e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   epoch_num  train_loss  val_acc  val_recall  val_precision    val_f1  \\\n",
              "0          1    0.668333    0.727     0.72752        0.72458  0.724107   \n",
              "\n",
              "         lr  \n",
              "0  0.000002  "
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "report_df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Control Experiment for 5 Epochs**"
      ],
      "metadata": {
        "id": "wfyZH_GJuBQX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2LdxjEQGIpG"
      },
      "outputs": [],
      "source": [
        "\n",
        "config_5={\"task_name\": \"AJGT_MARBERT\", #output directory name\n",
        "             \"data_dir\": \"./\", #data directory\n",
        "             \"train_file\": \"ar_rev_train.tsv\", #train file path\n",
        "             \"dev_file\": \"ar_rev_test.tsv\", #dev file path or test file path\n",
        "             \"pretrained_model_path\": 'MARBERT_pytorch_verison', #MARBERT checkpoint path\n",
        "             \"epochs\": 5, #number of epochs\n",
        "             \"content_col\": \"text\", #text column\n",
        "             \"label_col\": \"label\", #label column\n",
        "             \"lr\": 2e-06, #learning rate\n",
        "              \"max_seq_length\": 128, #max sequance length\n",
        "              \"batch_size\": 32, #batch shize\n",
        "              \"sortby\":\"val_acc\"} #sort results based on val_acc or val_f1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-n7vCBGZZKN_",
        "outputId": "16a6dd4d-6733-49b5-dedf-7fe2a8c45799"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] step (1) load train_test config file\n",
            "[INFO] step (2) convert labels2index\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading file vocab.txt\n",
            "loading file added_tokens.json\n",
            "loading file special_tokens_map.json\n",
            "loading file tokenizer_config.json\n",
            "loading configuration file MARBERT_pytorch_verison/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Mixed': 0, 'Negative': 1, 'Positive': 2}\n",
            "./AJGT_MARBERT_labels-dict.json\n",
            "[INFO] step (3) check checkpoint directory and report file\n",
            "[INFO] step (4) load label to number dictionary\n",
            "[INFO] train_file ./ar_rev_train.tsv\n",
            "[INFO] dev_file ./ar_rev_test.tsv\n",
            "[INFO] num_epochs 5\n",
            "[INFO] model_path MARBERT_pytorch_verison\n",
            "max_seq_length 128 batch_size 32\n",
            "[INFO] step (5) Use defined funtion to extract tokanize data\n",
            "loading BERT setting\n",
            "Data size  (79999, 2)\n",
            "The first sentence:\n",
            "[CLS] رواية بسيطه عن مغترب تونسي يزور عائلته في اجازه قصيره ويلمس تغير البلاد والافكار الدينيه التي غزت العقول وحجم التناقض الكبير بين القول والفعل ..رواية جميله على عكس روايته السابقه روائح ماري كلير .. [SEP]\n",
            "Tokenize the first sentence:\n",
            "['[CLS]', 'رواية', 'بسيطه', 'عن', 'مغترب', 'تونسي', 'يزور', 'عايلته', 'في', 'اجازه', 'قصيره', 'ويلم', '##س', 'تغير', 'البلاد', 'والافكار', 'الدينيه', 'التي', 'غز', '##ت', 'العقول', 'وحجم', 'التناقض', 'الكبير', 'بين', 'القول', 'والفعل', '.', '.', 'رواية', 'جميله', 'على', 'عكس', 'رواي', '##ته', 'السابقه', 'روايح', 'ماري', 'كلي', '##ر', '.', '.', '[SEP]']\n",
            "Index numbers of the first sentence:\n",
            "[2, 12268, 13009, 1986, 43299, 38656, 12203, 30919, 1947, 12042, 18794, 66258, 1019, 3969, 6812, 40324, 65586, 2367, 9461, 1009, 7849, 53462, 26215, 4935, 2199, 6915, 52491, 16, 16, 12268, 5551, 1977, 8124, 68846, 2088, 30577, 61165, 18682, 5248, 1010, 16, 16, 3]\n",
            "Index numbers of the first sentence after padding:\n",
            " [    2 12268 13009  1986 43299 38656 12203 30919  1947 12042 18794 66258\n",
            "  1019  3969  6812 40324 65586  2367  9461  1009  7849 53462 26215  4935\n",
            "  2199  6915 52491    16    16 12268  5551  1977  8124 68846  2088 30577\n",
            " 61165 18682  5248  1010    16    16     3     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0]\n",
            "Data size  (20000, 2)\n",
            "The first sentence:\n",
            "[CLS] أقم على الماشي . بصورة عامة الغرفة جيدة وكل شيء متوفر فيها. عدم وجود ستارة الحمام [SEP]\n",
            "Tokenize the first sentence:\n",
            "['[CLS]', 'اقم', 'على', 'الماشي', '.', 'بصورة', 'عامة', 'الغرفة', 'جيدة', 'وكل', 'شيء', 'متوفر', 'فيها', '.', 'عدم', 'وجود', 'ستار', '##ة', 'الحمام', '[SEP]']\n",
            "Index numbers of the first sentence:\n",
            "[2, 32246, 1977, 43114, 16, 6543, 8484, 21889, 13442, 2910, 2265, 11233, 2408, 16, 4113, 3715, 10709, 1046, 12169, 3]\n",
            "Index numbers of the first sentence after padding:\n",
            " [    2 32246  1977 43114    16  6543  8484 21889 13442  2910  2265 11233\n",
            "  2408    16  4113  3715 10709  1046 12169     3     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file MARBERT_pytorch_verison/config.json\n",
            "Model config BertConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.29.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "loading weights file MARBERT_pytorch_verison/pytorch_model.bin\n",
            "Some weights of the model checkpoint at MARBERT_pytorch_verison were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at MARBERT_pytorch_verison and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] step (6) Create an iterator of data with torch DataLoader.\n",
            "[INFO] step (7) run with parallel GPUs\n",
            "Run with one GPU\n",
            "[INFO] step (8) set Parameters, schedules, and loss function\n",
            "[INFO] step (9) start fine_tuning\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch:   0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "will return now\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Configuration saved in ./AJGT_MARBERT_bert_ckpt/model_1/config.json\n",
            "Model weights saved in ./AJGT_MARBERT_bert_ckpt/model_1/pytorch_model.bin\n",
            "Epoch:  20%|██        | 1/5 [30:35<2:02:21, 1835.42s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "will return now\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Configuration saved in ./AJGT_MARBERT_bert_ckpt/model_2/config.json\n",
            "Model weights saved in ./AJGT_MARBERT_bert_ckpt/model_2/pytorch_model.bin\n",
            "Epoch:  40%|████      | 2/5 [1:01:10<1:31:45, 1835.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "will return now\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Configuration saved in ./AJGT_MARBERT_bert_ckpt/model_3/config.json\n",
            "Model weights saved in ./AJGT_MARBERT_bert_ckpt/model_3/pytorch_model.bin\n",
            "Epoch:  60%|██████    | 3/5 [1:31:50<1:01:14, 1837.39s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "will return now\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Configuration saved in ./AJGT_MARBERT_bert_ckpt/model_4/config.json\n",
            "Model weights saved in ./AJGT_MARBERT_bert_ckpt/model_4/pytorch_model.bin\n",
            "Epoch:  80%|████████  | 4/5 [2:02:19<30:34, 1834.19s/it]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "will return now\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Configuration saved in ./AJGT_MARBERT_bert_ckpt/model_5/config.json\n",
            "Model weights saved in ./AJGT_MARBERT_bert_ckpt/model_5/pytorch_model.bin\n",
            "Epoch: 100%|██████████| 5/5 [2:32:47<00:00, 1833.46s/it]\n"
          ]
        }
      ],
      "source": [
        "report_df = fine_tuning(config_5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(report_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BgMm2OttMso",
        "outputId": "27d4648d-6b5d-45dd-d0f7-c8242f97a56d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   epoch_num  train_loss  val_acc  val_recall  val_precision    val_f1  \\\n",
            "0        1.0    0.726922  0.73035    0.730780       0.729674  0.730171   \n",
            "1        2.0    0.579407  0.73640    0.736757       0.737491  0.737082   \n",
            "2        3.0    0.539269  0.73830    0.738669       0.738982  0.738793   \n",
            "3        4.0    0.510306  0.73945    0.739751       0.740842  0.739712   \n",
            "4        5.0    0.492171  0.73860    0.739036       0.737268  0.737666   \n",
            "\n",
            "         lr  \n",
            "0  0.000002  \n",
            "1  0.000002  \n",
            "2  0.000002  \n",
            "3  0.000002  \n",
            "4  0.000002  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Spell Checked Experiment for 5 Epochs**"
      ],
      "metadata": {
        "id": "F5EjQAjyuJd5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "splitting data to train and test"
      ],
      "metadata": {
        "id": "8KscIc6_vJRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"spellchecked.tsv\", sep='\\t')\n",
        "df_text_label = df[['cleaned_no_sw', 'label']]\n",
        "print(df_text_label)\n",
        "train, test = train_test_split(df_text_label, test_size=0.2, random_state=42, shuffle=True)\n",
        "print(test)\n",
        "train.to_csv(\"spell_checked_train.tsv\", sep='\\t', index=False)\n",
        "test.to_csv(\"spell_checked_test.tsv\", sep='\\t', index=False)"
      ],
      "metadata": {
        "id": "9TLCtwUcxWKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbwuMLjjZPI_"
      },
      "outputs": [],
      "source": [
        "\n",
        "config_spellchecked={\"task_name\": \"AJGT_MARBERT\", #output directory name\n",
        "             \"data_dir\": \"./\", #data directory\n",
        "             \"train_file\": \"spell_checked_train.tsv\", #train file path\n",
        "             \"dev_file\": \"spell_checked_test.tsv\", #dev file path or test file path\n",
        "             \"pretrained_model_path\": 'MARBERT_pytorch_verison', #MARBERT checkpoint path\n",
        "             \"epochs\": 5, #number of epochs\n",
        "             \"content_col\": \"text\", #text column\n",
        "             \"label_col\": \"label\", #label column\n",
        "             \"lr\": 2e-06, #learning rate\n",
        "              \"max_seq_length\": 128, #max sequance length\n",
        "              \"batch_size\": 32, #batch shize\n",
        "              \"sortby\":\"val_acc\"} #sort results based on val_acc or val_f1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XH77XCkbpT2",
        "outputId": "92dece3c-8ceb-4794-81da-bb3443c0d3f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] step (1) load train_test config file\n",
            "[INFO] step (2) convert labels2index\n",
            "{'Mixed': 0, 'Negative': 1, 'Positive': 2}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading file vocab.txt\n",
            "loading file added_tokens.json\n",
            "loading file special_tokens_map.json\n",
            "loading file tokenizer_config.json\n",
            "loading configuration file MARBERT_pytorch_verison/config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "./AJGT_MARBERT_labels-dict.json\n",
            "[INFO] step (3) check checkpoint directory and report file\n",
            "[INFO] step (4) load label to number dictionary\n",
            "[INFO] train_file ./spell_checked_train.tsv\n",
            "[INFO] dev_file ./spell_checked_test.tsv\n",
            "[INFO] num_epochs 5\n",
            "[INFO] model_path MARBERT_pytorch_verison\n",
            "max_seq_length 128 batch_size 32\n",
            "[INFO] step (5) Use defined funtion to extract tokanize data\n",
            "loading BERT setting\n",
            "Data size  (79999, 2)\n",
            "The first sentence:\n",
            "[CLS] رواية بسيطه عن مغترب تونسي يزور عائلته في اجازه قصيره ويلمس تغير البلاد والافكار الدينيه التي غزت العقول وحجم التناقض الكبير بين القول والفعل ..رواية جميله على عكس روايته السابقه روائح ماري كلير .. [SEP]\n",
            "Tokenize the first sentence:\n",
            "['[CLS]', 'رواية', 'بسيطه', 'عن', 'مغترب', 'تونسي', 'يزور', 'عايلته', 'في', 'اجازه', 'قصيره', 'ويلم', '##س', 'تغير', 'البلاد', 'والافكار', 'الدينيه', 'التي', 'غز', '##ت', 'العقول', 'وحجم', 'التناقض', 'الكبير', 'بين', 'القول', 'والفعل', '.', '.', 'رواية', 'جميله', 'على', 'عكس', 'رواي', '##ته', 'السابقه', 'روايح', 'ماري', 'كلي', '##ر', '.', '.', '[SEP]']\n",
            "Index numbers of the first sentence:\n",
            "[2, 12268, 13009, 1986, 43299, 38656, 12203, 30919, 1947, 12042, 18794, 66258, 1019, 3969, 6812, 40324, 65586, 2367, 9461, 1009, 7849, 53462, 26215, 4935, 2199, 6915, 52491, 16, 16, 12268, 5551, 1977, 8124, 68846, 2088, 30577, 61165, 18682, 5248, 1010, 16, 16, 3]\n",
            "Index numbers of the first sentence after padding:\n",
            " [    2 12268 13009  1986 43299 38656 12203 30919  1947 12042 18794 66258\n",
            "  1019  3969  6812 40324 65586  2367  9461  1009  7849 53462 26215  4935\n",
            "  2199  6915 52491    16    16 12268  5551  1977  8124 68846  2088 30577\n",
            " 61165 18682  5248  1010    16    16     3     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0]\n",
            "Data size  (20000, 2)\n",
            "The first sentence:\n",
            "[CLS] أقم على الماشي . بصورة عامة الغرفة جيدة وكل شيء متوفر فيها. عدم وجود ستارة الحمام [SEP]\n",
            "Tokenize the first sentence:\n",
            "['[CLS]', 'اقم', 'على', 'الماشي', '.', 'بصورة', 'عامة', 'الغرفة', 'جيدة', 'وكل', 'شيء', 'متوفر', 'فيها', '.', 'عدم', 'وجود', 'ستار', '##ة', 'الحمام', '[SEP]']\n",
            "Index numbers of the first sentence:\n",
            "[2, 32246, 1977, 43114, 16, 6543, 8484, 21889, 13442, 2910, 2265, 11233, 2408, 16, 4113, 3715, 10709, 1046, 12169, 3]\n",
            "Index numbers of the first sentence after padding:\n",
            " [    2 32246  1977 43114    16  6543  8484 21889 13442  2910  2265 11233\n",
            "  2408    16  4113  3715 10709  1046 12169     3     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file MARBERT_pytorch_verison/config.json\n",
            "Model config BertConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.29.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "loading weights file MARBERT_pytorch_verison/pytorch_model.bin\n",
            "Some weights of the model checkpoint at MARBERT_pytorch_verison were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at MARBERT_pytorch_verison and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] step (6) Create an iterator of data with torch DataLoader.\n",
            "[INFO] step (7) run with parallel GPUs\n",
            "Run with one GPU\n",
            "[INFO] step (8) set Parameters, schedules, and loss function\n",
            "[INFO] step (9) start fine_tuning\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch:   0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "will return now\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Configuration saved in ./AJGT_MARBERT_bert_ckpt/model_1/config.json\n",
            "Model weights saved in ./AJGT_MARBERT_bert_ckpt/model_1/pytorch_model.bin\n",
            "Epoch:  20%|██        | 1/5 [31:49<2:07:19, 1909.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "will return now\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Configuration saved in ./AJGT_MARBERT_bert_ckpt/model_2/config.json\n",
            "Model weights saved in ./AJGT_MARBERT_bert_ckpt/model_2/pytorch_model.bin\n",
            "Epoch:  40%|████      | 2/5 [1:03:34<1:35:20, 1906.68s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "will return now\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Configuration saved in ./AJGT_MARBERT_bert_ckpt/model_3/config.json\n",
            "Model weights saved in ./AJGT_MARBERT_bert_ckpt/model_3/pytorch_model.bin\n",
            "Epoch:  60%|██████    | 3/5 [1:35:23<1:03:35, 1907.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "will return now\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Configuration saved in ./AJGT_MARBERT_bert_ckpt/model_4/config.json\n",
            "Model weights saved in ./AJGT_MARBERT_bert_ckpt/model_4/pytorch_model.bin\n",
            "Epoch:  80%|████████  | 4/5 [2:07:08<31:46, 1906.51s/it]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "will return now\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Configuration saved in ./AJGT_MARBERT_bert_ckpt/model_5/config.json\n",
            "Model weights saved in ./AJGT_MARBERT_bert_ckpt/model_5/pytorch_model.bin\n",
            "Epoch: 100%|██████████| 5/5 [2:38:57<00:00, 1907.47s/it]\n"
          ]
        }
      ],
      "source": [
        "report_df = fine_tuning(config_spellchecked)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(report_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ID5CPCBFtb8R",
        "outputId": "544f1fbd-8f98-4877-d502-a847f17d05b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   epoch_num  train_loss  val_acc  val_recall  val_precision    val_f1  \\\n",
            "0        1.0    0.722939  0.73055    0.730935       0.730665  0.730780   \n",
            "1        2.0    0.578857  0.73750    0.737840       0.738650  0.738193   \n",
            "2        3.0    0.539652  0.73760    0.737885       0.739714  0.738508   \n",
            "3        4.0    0.510117  0.73645    0.736740       0.738302  0.736791   \n",
            "4        5.0    0.490089  0.73535    0.735781       0.734194  0.734503   \n",
            "\n",
            "         lr  \n",
            "0  0.000002  \n",
            "1  0.000002  \n",
            "2  0.000002  \n",
            "3  0.000002  \n",
            "4  0.000002  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxkwAuuCT_Gs"
      },
      "source": [
        "# **MLSML Loss Function Experiment for 2 Epochs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nV2qeOzIBTAN"
      },
      "outputs": [],
      "source": [
        "\n",
        "def fine_tuning_MLSML(config):\n",
        "\t#---------------------------------------\n",
        "\tprint (\"[INFO] step (1) load train_test config file\")\n",
        "\t# config_file = open(config_file, 'r', encoding=\"utf8\")\n",
        "\t# config = json.load(config_file)\n",
        "\ttask_name = config[\"task_name\"]\n",
        "\tcontent_col = config[\"content_col\"]\n",
        "\tlabel_col = config[\"label_col\"]\n",
        "\ttrain_file = config[\"data_dir\"]+config[\"train_file\"]\n",
        "\tdev_file = config[\"data_dir\"]+config[\"dev_file\"]\n",
        "\tsortby = config[\"sortby\"]\n",
        "\tmax_seq_length= int(config[\"max_seq_length\"])\n",
        "\tbatch_size = int(config[\"batch_size\"])\n",
        "\tlr_var = float(config[\"lr\"])\n",
        "\tmodel_path = config['pretrained_model_path']\n",
        "\tnum_epochs = config['epochs'] # Number of training epochs (authors recommend between 2 and 4)\n",
        "\tglobal label2idx_file\n",
        "\tlabel2idx_file = config[\"data_dir\"]+config[\"task_name\"]+\"_labels-dict.json\"\n",
        "\t#-------------------------------------------------------\n",
        "\tprint (\"[INFO] step (2) convert labels2index\")\n",
        "\tcreate_label2ind_file(train_file, label_col)\n",
        "\tprint (label2idx_file)\n",
        "\t#---------------------------------------------------------\n",
        "\tprint (\"[INFO] step (3) check checkpoint directory and report file\")\n",
        "\tckpt_dir = config[\"data_dir\"]+task_name+\"_bert_ckpt/\"\n",
        "\treport = ckpt_dir+task_name+\"_report.tsv\"\n",
        "\tsorted_report = ckpt_dir+task_name+\"_report_sorted.tsv\"\n",
        "\tif not os.path.exists(ckpt_dir):\n",
        "\t\tos.mkdir(ckpt_dir)\n",
        "\t#-------------------------------------------------------\n",
        "\tprint (\"[INFO] step (4) load label to number dictionary\")\n",
        "\tlab2ind = json.load(open(label2idx_file))\n",
        "\tprint (\"[INFO] train_file\", train_file)\n",
        "\tprint (\"[INFO] dev_file\", dev_file)\n",
        "\tprint (\"[INFO] num_epochs\", num_epochs)\n",
        "\tprint (\"[INFO] model_path\", model_path)\n",
        "\tprint (\"max_seq_length\", max_seq_length, \"batch_size\", batch_size)\n",
        "\t#-------------------------------------------------------\n",
        "\tprint (\"[INFO] step (5) Use defined funtion to extract tokanize data\")\n",
        "\t# tokenizer from pre-trained BERT model\n",
        "\tprint (\"loading BERT setting\")\n",
        "\ttokenizer = BertTokenizer.from_pretrained(model_path)\n",
        "\ttrain_inputs, train_labels, train_masks = data_prepare_BERT(train_file, lab2ind, tokenizer,content_col, label_col, max_seq_length)\n",
        "\tvalidation_inputs, validation_labels, validation_masks = data_prepare_BERT(dev_file, lab2ind, tokenizer, content_col, label_col,max_seq_length)\n",
        "\t# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top.\n",
        "\tmodel = BertForSequenceClassification.from_pretrained(model_path, num_labels=len(lab2ind))\n",
        "\t#--------------------------------------\n",
        "\tprint (\"[INFO] step (6) Create an iterator of data with torch DataLoader.\")\n",
        "#\t\t  This helps save on memory during training because, unlike a for loop,\\\n",
        "#\t\t  with an iterator the entire dataset does not need to be loaded into memory\")\n",
        "\ttrain_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "\ttrain_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
        "\t#---------------------------\n",
        "\tvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "\tvalidation_dataloader = DataLoader(validation_data, batch_size=batch_size)\n",
        "\t#------------------------------------------\n",
        "\tprint (\"[INFO] step (7) run with parallel GPUs\")\n",
        "\tif torch.cuda.is_available():\n",
        "\t\tif torch.cuda.device_count() == 1:\n",
        "\t\t\tprint(\"Run\", \"with one GPU\")\n",
        "\t\t\tmodel = model.to(device)\n",
        "\t\telse:\n",
        "\t\t\tn_gpu = torch.cuda.device_count()\n",
        "\t\t\tprint(\"Run\", \"with\", n_gpu, \"GPUs with max 4 GPUs\")\n",
        "\t\t\tdevice_ids = GPUtil.getAvailable(limit = 4)\n",
        "\t\t\ttorch.backends.cudnn.benchmark = True\n",
        "\t\t\tmodel = model.to(device)\n",
        "\t\t\tmodel = nn.DataParallel(model, device_ids=device_ids)\n",
        "\telse:\n",
        "\t\tprint(\"Run\", \"with CPU\")\n",
        "\t\tmodel = model\n",
        "\t#---------------------------------------------------\n",
        "\tprint (\"[INFO] step (8) set Parameters, schedules, and loss function\")\n",
        "\tglobal max_grad_norm\n",
        "\tmax_grad_norm = 1.0\n",
        "\twarmup_proportion = 0.1\n",
        "\tnum_training_steps\t= len(train_dataloader) * num_epochs\n",
        "\tnum_warmup_steps = num_training_steps * warmup_proportion\n",
        "\t### In Transformers, optimizer and schedules are instantiated like this:\n",
        "\t# Note: AdamW is a class from the huggingface library\n",
        "\t# the 'W' stands for 'Weight Decay\"\n",
        "\toptimizer = AdamW(model.parameters(), lr=lr_var, correct_bias=False)\n",
        "\t# schedules\n",
        "\tscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)  # PyTorch scheduler\n",
        "\t# We use nn.MultiLabelSoftMarginLoss() as our loss function. \n",
        "\tcriterion = nn.MultiLabelSoftMarginLoss(weight=None, size_average=None, reduce=None, reduction='mean')\n",
        "\t#---------------------------------------------------\n",
        "\tprint (\"[INFO] step (9) start fine_tuning\")\n",
        "\tfor epoch in trange(num_epochs, desc=\"Epoch\"):\n",
        "\t\ttrain_loss = train(model, train_dataloader, optimizer, scheduler, criterion)\t  \n",
        "\t\tval_loss, val_acc, val_f1, val_recall, val_precision = evaluate(model, validation_dataloader, criterion)\n",
        "# \t\tprint (train_loss, val_acc)\n",
        "\t\t# Create checkpoint at end of each epoch\n",
        "\t\tif not os.path.exists(ckpt_dir + 'model_' + str(int(epoch + 1)) + '/'): os.mkdir(ckpt_dir + 'model_' + str(int(epoch + 1)) + '/')\n",
        "\t\tmodel.save_pretrained(ckpt_dir+ 'model_' + str(int(epoch + 1)) + '/')\n",
        "\t\tepoch_eval_results = {\"epoch_num\":int(epoch + 1),\"train_loss\":train_loss,\n",
        "\t\t\t\t\t  \"val_acc\":val_acc, \"val_recall\":val_recall, \"val_precision\":val_precision, \"val_f1\":val_f1,\"lr\":lr_var }\n",
        "\t\tprint(epoch_eval_results)\n",
        "\t\twith open(report,\"a\") as fOut:\n",
        "\t\t\tfOut.write(json.dumps(epoch_eval_results)+\"\\n\")\n",
        "\t\t\tfOut.flush()\n",
        "\t\t#------------------------------------\n",
        "\t\treport_df = pd.read_json(report, orient='records', lines=True)\n",
        "\t\treport_df.sort_values(by=[sortby],ascending=False, inplace=True)\n",
        "\t\treport_df.to_csv(sorted_report,sep=\"\\t\",index=False)\n",
        "\treturn report_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "32XYIdPBUHOF",
        "outputId": "57e3df65-ca91-472c-fcb1-1e0e3925aacf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] step (1) load train_test config file\n",
            "[INFO] step (2) convert labels2index\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading file vocab.txt\n",
            "loading file added_tokens.json\n",
            "loading file special_tokens_map.json\n",
            "loading file tokenizer_config.json\n",
            "loading configuration file MARBERT_pytorch_verison/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Mixed': 0, 'Negative': 1, 'Positive': 2}\n",
            "./AJGT_MARBERT_labels-dict.json\n",
            "[INFO] step (3) check checkpoint directory and report file\n",
            "[INFO] step (4) load label to number dictionary\n",
            "[INFO] train_file ./ar_rev_train.tsv\n",
            "[INFO] dev_file ./ar_rev_test.tsv\n",
            "[INFO] num_epochs 5\n",
            "[INFO] model_path MARBERT_pytorch_verison\n",
            "max_seq_length 128 batch_size 32\n",
            "[INFO] step (5) Use defined funtion to extract tokanize data\n",
            "loading BERT setting\n",
            "Data size  (79999, 2)\n",
            "The first sentence:\n",
            "[CLS] رواية بسيطه عن مغترب تونسي يزور عائلته في اجازه قصيره ويلمس تغير البلاد والافكار الدينيه التي غزت العقول وحجم التناقض الكبير بين القول والفعل ..رواية جميله على عكس روايته السابقه روائح ماري كلير .. [SEP]\n",
            "Tokenize the first sentence:\n",
            "['[CLS]', 'رواية', 'بسيطه', 'عن', 'مغترب', 'تونسي', 'يزور', 'عايلته', 'في', 'اجازه', 'قصيره', 'ويلم', '##س', 'تغير', 'البلاد', 'والافكار', 'الدينيه', 'التي', 'غز', '##ت', 'العقول', 'وحجم', 'التناقض', 'الكبير', 'بين', 'القول', 'والفعل', '.', '.', 'رواية', 'جميله', 'على', 'عكس', 'رواي', '##ته', 'السابقه', 'روايح', 'ماري', 'كلي', '##ر', '.', '.', '[SEP]']\n",
            "Index numbers of the first sentence:\n",
            "[2, 12268, 13009, 1986, 43299, 38656, 12203, 30919, 1947, 12042, 18794, 66258, 1019, 3969, 6812, 40324, 65586, 2367, 9461, 1009, 7849, 53462, 26215, 4935, 2199, 6915, 52491, 16, 16, 12268, 5551, 1977, 8124, 68846, 2088, 30577, 61165, 18682, 5248, 1010, 16, 16, 3]\n",
            "Index numbers of the first sentence after padding:\n",
            " [    2 12268 13009  1986 43299 38656 12203 30919  1947 12042 18794 66258\n",
            "  1019  3969  6812 40324 65586  2367  9461  1009  7849 53462 26215  4935\n",
            "  2199  6915 52491    16    16 12268  5551  1977  8124 68846  2088 30577\n",
            " 61165 18682  5248  1010    16    16     3     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0]\n",
            "Data size  (20000, 2)\n",
            "The first sentence:\n",
            "[CLS] أقم على الماشي . بصورة عامة الغرفة جيدة وكل شيء متوفر فيها. عدم وجود ستارة الحمام [SEP]\n",
            "Tokenize the first sentence:\n",
            "['[CLS]', 'اقم', 'على', 'الماشي', '.', 'بصورة', 'عامة', 'الغرفة', 'جيدة', 'وكل', 'شيء', 'متوفر', 'فيها', '.', 'عدم', 'وجود', 'ستار', '##ة', 'الحمام', '[SEP]']\n",
            "Index numbers of the first sentence:\n",
            "[2, 32246, 1977, 43114, 16, 6543, 8484, 21889, 13442, 2910, 2265, 11233, 2408, 16, 4113, 3715, 10709, 1046, 12169, 3]\n",
            "Index numbers of the first sentence after padding:\n",
            " [    2 32246  1977 43114    16  6543  8484 21889 13442  2910  2265 11233\n",
            "  2408    16  4113  3715 10709  1046 12169     3     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file MARBERT_pytorch_verison/config.json\n",
            "Model config BertConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.29.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "loading weights file MARBERT_pytorch_verison/pytorch_model.bin\n",
            "Some weights of the model checkpoint at MARBERT_pytorch_verison were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at MARBERT_pytorch_verison and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] step (6) Create an iterator of data with torch DataLoader.\n",
            "[INFO] step (7) run with parallel GPUs\n",
            "Run with one GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] step (8) set Parameters, schedules, and loss function\n",
            "[INFO] step (9) start fine_tuning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:   0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "will return now\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in ./AJGT_MARBERT_bert_ckpt/model_1/config.json\n",
            "Model weights saved in ./AJGT_MARBERT_bert_ckpt/model_1/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'epoch_num': 1, 'train_loss': 0.728766883111, 'val_acc': 0.7311, 'val_recall': 0.7315776732115117, 'val_precision': 0.7291082453475847, 'val_f1': 0.7299571890529286, 'lr': 2e-06}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  20%|██        | 1/5 [30:56<2:03:47, 1856.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "will return now\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in ./AJGT_MARBERT_bert_ckpt/model_2/config.json\n",
            "Model weights saved in ./AJGT_MARBERT_bert_ckpt/model_2/pytorch_model.bin\n",
            "Epoch:  40%|████      | 2/5 [1:01:56<1:32:55, 1858.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'epoch_num': 2, 'train_loss': 0.5787095896303653, 'val_acc': 0.7362, 'val_recall': 0.7365670273223827, 'val_precision': 0.7367135054173034, 'val_f1': 0.7365874601617136, 'lr': 2e-06}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  40%|████      | 2/5 [1:24:18<2:06:28, 2529.33s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-09df161153da>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreport_df_mlsml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfine_tuning_MLSML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-41-48a21b1ea9fd>\u001b[0m in \u001b[0;36mfine_tuning_MLSML\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"[INFO] step (9) start fine_tuning\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Epoch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m                 \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m                 \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_f1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_recall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_precision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;31m#               print (train_loss, val_acc)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-ca80e2304b6f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, scheduler, criterion)\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m                         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "report_df_mlsml = fine_tuning_MLSML(config_5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df3=pd.DataFrame(columns=['epoch_num',    'train_loss',    'val_acc',    'val_recall',    'val_precision',    'val_f1',    'lr'])\n",
        "df3.loc[len(df3)]=[1,  0.728766883111,  0.7311, 0.7315776732115117,  0.7291082453475847, 0.7299571890529286,  2e-06]\n",
        "df3.loc[len(df3)]=[2, 0.5787095896303653,  0.7362,  0.7365670273223827,  0.7367135054173034, 0.7365874601617136,  2e-06]\n"
      ],
      "metadata": {
        "id": "u2mrRtI0tjZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df3)"
      ],
      "metadata": {
        "id": "542o0kpUtmB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "splitting data into training and testing"
      ],
      "metadata": {
        "id": "6cbHRC_cueL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"no_sw.tsv\", sep='\\t')\n",
        "df_text_label = df[['cleaned_no_sw', 'label']]\n",
        "print(df_text_label)\n",
        "train, test = train_test_split(df_text_label, test_size=0.2, random_state=42, shuffle=True)\n",
        "print(test)\n",
        "train.to_csv(\"no_sw_train.tsv\", sep='\\t', index=False)\n",
        "test.to_csv(\"no_sw_test.tsv\", sep='\\t', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MK42kDKUX_1T",
        "outputId": "10dc0a70-222c-418a-c67f-4c1ceb96b103"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                           cleaned_no_sw     label\n",
            "0      ممتاز نوعا . النظافة والموقع التجهيز الشاطيء ....  Positive\n",
            "1      أحد أسباب نجاح الإمارات شخص الدولة يعشق تراها ...  Positive\n",
            "2      هادفة وقوية . تنقلك صخب شوارع القاهرة الى هدوء...  Positive\n",
            "3      خلصنا مبدئيا اللي مستوي انهار زي الفيل الازرق ...  Positive\n",
            "4      سياسات جلوريا جزء لا يتجزأ دبي . فندق متكامل ا...  Positive\n",
            "...                                                  ...       ...\n",
            "99994  معرفش ليه كنت عاوزة أكملها مش ياباني البداية ا...  Negative\n",
            "99995  لا يستحق ان يكون وكنت لانه سيئ . لا شي . لا يو...  Negative\n",
            "99996  كتاب ضعيف استمتع . فى قصه سرد لحاله مشهد فكره قصه  Negative\n",
            "99997  . محمد حسن عنوان فنان بالكلمات والنصف عندة دقي...  Negative\n",
            "99998  ارجع اخرى . قربه البحر . المكان قديم ولا توجد ...  Negative\n",
            "\n",
            "[99999 rows x 2 columns]\n",
            "                                           cleaned_no_sw     label\n",
            "26002  أقم الماشي . بصورة الغرفة جيدة شيء متوفر . عدم...  Positive\n",
            "80420     تعجبني كباقي السلسلة متحمس لقراءة الجزء الثاني  Negative\n",
            "19864  المكان ممتاز والأمن والاستقبال اوكي . . التكيي...  Positive\n",
            "81525  القصة جمالا مشوقة كونها تعتمد شخصيات يجمع مكان...  Negative\n",
            "57878  اربع نجوم الملل اصابني النهاية رواية جميلة بال...     Mixed\n",
            "...                                                  ...       ...\n",
            "99336  يعيب الكتاب مسيطرة فكرة كإمرأة بعملي شئ شئ إرض...  Negative\n",
            "29311  شكرا لطاقم العمل . الفندق قريب الحرم والخدمة م...  Positive\n",
            "97599                        مرجع مهم موفق لحرب كوابيسها  Negative\n",
            "61294  جميل ورايت لناس الرايه لايصلح تروح عطلة مدارس ...     Mixed\n",
            "84226  مخيب لأجل . موقع الفندق بعيد ضجيج السيارات . ا...  Negative\n",
            "\n",
            "[20000 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "config_no_sw={\"task_name\": \"AJGT_MARBERT\", #output directory name\n",
        "             \"data_dir\": \"./\", #data directory\n",
        "             \"train_file\": \"no_sw_train.tsv\", #train file path\n",
        "             \"dev_file\": \"no_sw_test.tsv\", #dev file path or test file path\n",
        "             \"pretrained_model_path\": 'MARBERT_pytorch_verison', #MARBERT checkpoint path\n",
        "             \"epochs\": 2, #number of epochs\n",
        "             \"content_col\": \"cleaned_no_sw\", #text column\n",
        "             \"label_col\": \"label\", #label column\n",
        "             \"lr\": 2e-06, #learning rate\n",
        "              \"max_seq_length\": 128, #max sequance length\n",
        "              \"batch_size\": 32, #batch shize\n",
        "              \"sortby\":\"val_acc\"} #sort results based on val_acc or val_f1\n"
      ],
      "metadata": {
        "id": "HWRdGnHNV2pQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "report_df_stopwords = fine_tuning(config_no_sw)"
      ],
      "metadata": {
        "id": "ChheYjXAV99d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e87c4f16-bead-4010-9546-28fbbdc55678"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] step (1) load train_test config file\n",
            "[INFO] step (2) convert labels2index\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading file vocab.txt\n",
            "loading file added_tokens.json\n",
            "loading file special_tokens_map.json\n",
            "loading file tokenizer_config.json\n",
            "loading configuration file MARBERT_pytorch_verison/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Mixed': 0, 'Negative': 1, 'Positive': 2}\n",
            "./AJGT_MARBERT_labels-dict.json\n",
            "[INFO] step (3) check checkpoint directory and report file\n",
            "[INFO] step (4) load label to number dictionary\n",
            "[INFO] train_file ./no_sw_train.tsv\n",
            "[INFO] dev_file ./no_sw_test.tsv\n",
            "[INFO] num_epochs 2\n",
            "[INFO] model_path MARBERT_pytorch_verison\n",
            "max_seq_length 128 batch_size 32\n",
            "[INFO] step (5) Use defined funtion to extract tokanize data\n",
            "loading BERT setting\n",
            "Data size  (79983, 2)\n",
            "The first sentence:\n",
            "[CLS] رواية بسيطه مغتصب توني يزور عائلته اجازه قصيره يلمس تغير البلاد الافكار الدينيه غزت العقول وحجم التناقض الكبير القول بالفعل رواية جميله عكس روايته السابقه روائح ماري كلير [SEP]\n",
            "Tokenize the first sentence:\n",
            "['[CLS]', 'رواية', 'بسيطه', 'مغتصب', 'توني', 'يزور', 'عايلته', 'اجازه', 'قصيره', 'يلمس', 'تغير', 'البلاد', 'الافكار', 'الدينيه', 'غز', '##ت', 'العقول', 'وحجم', 'التناقض', 'الكبير', 'القول', 'بالفعل', 'رواية', 'جميله', 'عكس', 'رواي', '##ته', 'السابقه', 'روايح', 'ماري', 'كلي', '##ر', '[SEP]']\n",
            "Index numbers of the first sentence:\n",
            "[2, 12268, 13009, 72760, 12284, 12203, 30919, 12042, 18794, 40932, 3969, 6812, 10437, 65586, 9461, 1009, 7849, 53462, 26215, 4935, 6915, 12632, 12268, 5551, 8124, 68846, 2088, 30577, 61165, 18682, 5248, 1010, 3]\n",
            "Index numbers of the first sentence after padding:\n",
            " [    2 12268 13009 72760 12284 12203 30919 12042 18794 40932  3969  6812\n",
            " 10437 65586  9461  1009  7849 53462 26215  4935  6915 12632 12268  5551\n",
            "  8124 68846  2088 30577 61165 18682  5248  1010     3     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0]\n",
            "Data size  (19998, 2)\n",
            "The first sentence:\n",
            "[CLS] أقم الماشي . بصورة الغرفة جيدة شيء متوفر . عدم وجود ستارة الحمام [SEP]\n",
            "Tokenize the first sentence:\n",
            "['[CLS]', 'اقم', 'الماشي', '.', 'بصورة', 'الغرفة', 'جيدة', 'شيء', 'متوفر', '.', 'عدم', 'وجود', 'ستار', '##ة', 'الحمام', '[SEP]']\n",
            "Index numbers of the first sentence:\n",
            "[2, 32246, 43114, 16, 6543, 21889, 13442, 2265, 11233, 16, 4113, 3715, 10709, 1046, 12169, 3]\n",
            "Index numbers of the first sentence after padding:\n",
            " [    2 32246 43114    16  6543 21889 13442  2265 11233    16  4113  3715\n",
            " 10709  1046 12169     3     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file MARBERT_pytorch_verison/config.json\n",
            "Model config BertConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.29.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 100000\n",
            "}\n",
            "\n",
            "loading weights file MARBERT_pytorch_verison/pytorch_model.bin\n",
            "Some weights of the model checkpoint at MARBERT_pytorch_verison were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at MARBERT_pytorch_verison and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] step (6) Create an iterator of data with torch DataLoader.\n",
            "[INFO] step (7) run with parallel GPUs\n",
            "Run with one GPU\n",
            "[INFO] step (8) set Parameters, schedules, and loss function\n",
            "[INFO] step (9) start fine_tuning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:   0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "will return now\n",
            "0.7258054706215858 0.7090709070907091\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in ./AJGT_MARBERT_bert_ckpt/model_1/config.json\n",
            "Model weights saved in ./AJGT_MARBERT_bert_ckpt/model_1/pytorch_model.bin\n",
            "Epoch:  50%|█████     | 1/2 [31:20<31:20, 1880.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "will return now\n",
            "0.6160594448089599 0.7107210721072107\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in ./AJGT_MARBERT_bert_ckpt/model_2/config.json\n",
            "Model weights saved in ./AJGT_MARBERT_bert_ckpt/model_2/pytorch_model.bin\n",
            "Epoch: 100%|██████████| 2/2 [1:02:35<00:00, 1877.99s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "report_df_stopwords[19:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "-6ZhBHf0bVj0",
        "outputId": "cc7768a7-21de-4162-f0bb-d60aa5592608"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    epoch_num  train_loss   val_acc  val_recall  val_precision    val_f1  \\\n",
              "20          2    0.616059  0.710721    0.711175       0.708451  0.708701   \n",
              "\n",
              "          lr  \n",
              "20  0.000002  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b923a950-716c-4807-a959-d46814371e47\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>epoch_num</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>val_acc</th>\n",
              "      <th>val_recall</th>\n",
              "      <th>val_precision</th>\n",
              "      <th>val_f1</th>\n",
              "      <th>lr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>2</td>\n",
              "      <td>0.616059</td>\n",
              "      <td>0.710721</td>\n",
              "      <td>0.711175</td>\n",
              "      <td>0.708451</td>\n",
              "      <td>0.708701</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b923a950-716c-4807-a959-d46814371e47')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b923a950-716c-4807-a959-d46814371e47 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b923a950-716c-4807-a959-d46814371e47');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Confusion Matrix Code**"
      ],
      "metadata": {
        "id": "yGp8AB1H0upy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conf_mat(model, iterator, criterion):\n",
        "  model.eval()\n",
        "  epoch_loss = 0\n",
        "  all_pred=[]\n",
        "  all_label = []\n",
        "  with torch.no_grad():\n",
        "    for i, batch in enumerate(iterator):\n",
        "      # Add batch to GPU\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      # Unpack the inputs from our dataloader\n",
        "      input_ids, input_mask, labels = batch\n",
        "      outputs = model(input_ids, input_mask, labels=labels)\n",
        "      loss, logits = outputs[:2]\n",
        "      # delete used variables to free GPU memory\n",
        "      del batch, input_ids, input_mask\n",
        "      if torch.cuda.device_count() == 1:\n",
        "        epoch_loss += loss.cpu().item()\n",
        "      else:\n",
        "        epoch_loss += loss.sum().cpu().item()\n",
        "      # identify the predicted class for each example in the batch\n",
        "      probabilities, predicted = torch.max(logits.cpu().data, 1)\n",
        "      # put all the true labels and predictions to two lists\n",
        "      all_pred.extend(predicted)\n",
        "      all_label.extend(labels.cpu())\n",
        "    \n",
        "  tp=0\n",
        "  fp=0\n",
        "  tn=0\n",
        "  fn=0\n",
        "  for p,l in all_pred, all_label:\n",
        "    if p==l and p==\"Positive\":\n",
        "      tp=tp+1\n",
        "    if p==l and p==\"Negative\":\n",
        "      tn=tn+1\n",
        "    if p!=l and p==\"Negative\":\n",
        "      fn=fn+1\n",
        "    if p!=l and p==\"Positive\":\n",
        "      fp=fp+1\n",
        "\n",
        "  return tp,tn,fp,fn"
      ],
      "metadata": {
        "id": "v9HOnMPsyBAJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
